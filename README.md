# ğŸ“Œ VideoRAG: Video-Based Retrieval-Augmented Generation

## ğŸ“– Overview
VideoRAG is a Retrieval-Augmented Generation (RAG) system that processes video content by generating captions and answering user queries based on extracted text. It leverages **Whisper**, **sentence-transformers**, and **Qdrant** to store and retrieve relevant information from videos efficiently.

## âœ¨ Features
- **Automatic Captioning:** Uses OpenAIâ€™s **Whisper** model to generate subtitles for videos.
- **Semantic Search & Retrieval:** Stores extracted text embeddings in **Qdrant** for efficient retrieval.
- **Answer Generation:** Leverages **transformers** to generate answers based on retrieved context.
- **User-Friendly Interface:** Built using **Streamlit** for easy interaction.

## ğŸ“‚ Project Structure
```
VideoRAG-Project/
â”‚â”€â”€ data/                      # Folder for storing input videos and related data  
â”‚â”€â”€ outputs/                   # Folder for storing generated outputs (captions, text, answers)  
â”‚â”€â”€ script/                    # All Python scripts for processing  
â”‚   â”‚â”€â”€ main.py                # Main script to run the VideoRAG system  
â”‚   â”‚â”€â”€ generate_captions.py    # Generates captions from video using Whisper  
â”‚   â”‚â”€â”€ store_embedding.py       # Converts text into embeddings using sentence-transformers  
â”‚   â”‚â”€â”€ retrieval.py            # Handles RAG-based retrieval using Qdrant  
â”‚   â”‚â”€â”€ answer_generation.py     # Generates answers using transformers  
â”‚   â”‚â”€â”€ app.py                   # Streamlit-based user interface for interaction
â”‚   â”‚â”€â”€ search_video.py          # Search video using CLIP or text queries
|   â”‚â”€â”€extract_frames.py         # Extracts frames from video
|   â”‚â”€â”€ extract_audio.py          # Converts video to audio
|   â”‚â”€â”€ transcribe_audio.py       # Uses Whisper for speech-to-text
â”‚â”€â”€ venv/                      # Virtual environment (not included in repo)  
â”‚â”€â”€ requirements.txt           # Required dependencies  
â”‚â”€â”€ README.md                  # Project documentation  
â”‚â”€â”€ .gitignore                 # Ignore unnecessary files (e.g., venv, dataset caches)  
```

## ğŸš€ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/VideoRAG.git  
cd VideoRAG
```

### 2ï¸âƒ£ Create & Activate a Virtual Environment
```bash
python -m venv venv  
source venv/bin/activate  # On macOS/Linux  
venv\Scripts\activate     # On Windows  
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt  
```

## ğŸ”¥ Usage

### Run the Streamlit UI:
```bash
streamlit run script/ui.py  
```

### Run the Main Processing Script:
```bash
python script/main.py --video_path "data/sample.mp4"
```

## ğŸ›  Dependencies (`requirements.txt`)
```
qdrant-client  
streamlit  
answer-generation  
sentence-transformers  
transformers  
torch  
openai-whisper  
```

## ğŸ“Œ Future Improvements
- ğŸ”¹ Integrate **multi-modal models** for richer video analysis.  
- ğŸ”¹ Enhance retrieval accuracy using **fine-tuned embeddings**.  
- ğŸ”¹ Deploy as a **web-based application** for easier access.  

## ğŸ“œ License
This project is licensed under the MIT License.

---
### ğŸ¤ Contributing
Contributions are welcome! Feel free to submit a PR or open an issue. ğŸ˜Š  

---
ğŸš€ **Developed with â¤ï¸ by [Roshani Singh]**

